### note
origin github addr is https://github.com/thohemp/6DRepNet.git<br>
This project is based on the original project for modification, seamlessly supporting the tool from the repository: https://github.com/HelloWorld158/HeadPoseAnnotation.git. It has slightly modified the model architecture by adding a channel responsible for predicting the confidence of generating correct rotation amounts. When the confidence is very low, this rotation amount is considered unreliable.<br>
### install
pip install -r requirements.txt<br>
### training
Download pre-trained RepVGG model '**RepVGG-B1g2-train.pth**' from [here](https://drive.google.com/drive/folders/1Avome4KvNp0Lqh2QwhXO6L5URQjzCjUq)<br>
Place the pre-trained weights in the models folder.<br>
Place the annotation files generated by the tool from https://github.com/HelloWorld158/HeadPoseAnnotation.git into the headDir/train and headDir/val folders for training and validation, respectively.<br>
python sixdrepnet/train.py --num_epochs 100 --batch_size 8 train.py This code is relatively simple and can be modified directly.<br>
### Inference
å°†https://github.com/open-mmlab/mmdetection.git and https://github.com/open-mmlab/mmyolo.git Put the contents of these two files separately into MMDetecTVT/mmdetection and MMDetecTVT/mmyolodet <br>
Place the JPG images under valTestData and then run python inferwithdetect.py<br>